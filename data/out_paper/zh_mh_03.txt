
 这段文字已经是中文翻译了。
 B. 在对象检测中
对象检测是在图像中识别各种对象（如汽车、飞机、狗和猫）并定位它们，通常使用每个对象实例周围的边界框。已经开发了许多优秀的对象检测模型[8]。它们主要分为两类：两阶段对象检测器和单阶段对象检测器。两阶段检测器（如 R-CNN [9]，SPP-net [10] 和 DetectoRS [11]）的第一步是创建区域建议，第二步进一步细化提议的边界框并进行分类。而单阶段检测器直接生成边界框和类别，无需区域建议。例如 YOLO [12]，SSD [13]，和 EfficientDet [14]。总的来说，两阶段检测器的准确性高于单阶段检测器，但在实时场景中适用性较低。

尽管这些解决方案在机器人感知[15]和静态感知[16]方面很常见，但只有少数研究评估了它们的潜力以理解用户行为。Geert Br ˆone等人[17]将注视数据与对象检测算法相结合，以证明使用检测算法进行自动注视数据分析的有效性。他们还设计了一个概念证明，但没有报告结果。另一项研究[18]探讨了使用两个检测模型（YOLOv2 和 OpenPose）将目光位置特定于人类面对面交流中参与者可见头部和手部的关系。

一项更近期的研究工作[19]在合成数据上训练了一个分类模型，将其裁剪成围绕目光位置的图像作为标注感兴趣体素的方法。这种方法的问题在于，图像（或图像裁剪）将被分配只有一个类别。即使它包含多个对象，也会被标记为最显著对象的图像，而忽略了一个人可能同时看着距离相近的多个物体的情况。相比之下，对象检测通过识别图像中的所有对象而不是仅仅最显著的对象，来传递此类信息具有潜力。同样，[20]也使用了分类模型和图像裁剪，并将它与使用对象检测模型进行了比较。然而，他们仅依靠可用的预训练模型，没有针对特定应用进行调整。

与那些先前的作品不同，我们的工作是一个针对现实场景的应用进行优化和调整的模型，并使用 UA V 飞行器提取有意义的用户行为数据。

接近我们研究兴趣的是 Miller 等人[21]，他们对预训练的 InceptionV2 检测模型进行微调，以在眼动视频数据中标注感兴趣的对象，并将它与身体动作捕捉数据相结合。他们的目标是将目光位置与参与者的身体位置和其他帧内的对象相关联。此外，他们在一个用户研究中部署了他们的方法，以研究目光点与目标对象之间的距离。我们的工作与他们不同，因为在应用场景中，我们部署的工具（学习模型和生物物理测量值）以及我们对用户研究的深入分析。

最后，对象检测需要大量的处理能力：在 30 fps 下运行最先进的检测算法几乎无法在车载设备、可穿戴设备甚至更强大的机器人计算机上实现。以前进行的性能测试是在实时环境中对 YOLO 变体进行无人机紧急着陆的mean average precision (mAP) 和帧率（FPS）。有两个模型表现出色：YOLOv3 的速度和 YOLOv5 的准确性。

III. 操作生理学

图1。我们在HRI4ROS管道中的贡献显示为ROS rqt图形。输入节点在左侧。每个节点在标准命名空间下发布一组主题。

生理信号捕获和分析是人体中心机器人系统设计和验证的基本要素。许多研究都证明了这些测量的相关性，但每一种都有其自己的实现方式。因此，准确地复制这些研究并将其应用于其他环境可能需要花费很长时间。

在机器人学领域，机器人操作系统（ROS）将自己视为代码和软件基础设施的标准，以共享和合作。它的社区正在逐步将用户基于的测量和可穿戴设备驱动程序集成到 ROS 中，以应对人类机器人交互（HRI）社区的可重复性和分享挑战[23]。目前正在进行一个将 HRI 考虑纳入 ROS 的标准[24]。

我们的工作旨在通过整合新的传感器模态，为标准社区做出贡献。该标准建议将与人相关的因素划分为五个子命名空间。然而，为了更好地适应 pupilometry 数据结构，我们将子命名空间 /human/faces/<face\_id> 分到了另一个子类别，即眼睛。眼睛数据是与注视、瞳孔直径和眨眼有关的。注视消息（2D 和 3D）由 geometry msgs/Point 和两个时间戳组成：一个是 ROS 的，另一个是时间戳。
 其他从设备（如有）获取。瞳孔消息（原始和过滤）将直径值以毫米为单位作为浮点数与相同两个时间戳发布。瞳孔驱动程序考虑使用基于WiFi的网络socket连接，兼容于几个品牌，如Tobii和PupilLabs。大多数瞳孔测量商业设备还提供眼睛相机和场景相机的实时流，提供一个用户视角的前方相机视图。另一个节点订阅这些流并在ROS图像主题中转换它们。

第二感知模态基于心电图信号。驱动程序提供/humans/bodies/<身体ID>命名空间中的原始ECG值（单位为mV）。我们还提供从信号中提取的更高级功能：RR间隔和每分钟的心跳次数。呼吸率也可以从心电图信号中提取，或者如果有可用性，则从胸部加速度计位置中提取。所有这些数据都作为打标的浮点数ROS消息发布。

图1说明了我们在ROS4HRI管道中做出的贡献[24]，通过添加这两个新的感知模式。每个新用户（或机器人操作者）都分配一个唯一的ID，基于可用的模块来执行人员识别。当前HRI4ROS管道[23]建议使用面部识别，但您可以使用我们提供的瞳孔形状作为独特的生物识别符。传感器参考框架可以通过几何变换（TF）消息与身体链接。当使用设备时，这些值都实时发布，但它们可以轻松地记录在ROSbag中进行后处理和数据分析。但是，为了实时监控数据质量并为主记录一些关键点，我们还开发了一个Jupyter笔记本可视化界面。所有代码都可以在线获得1。

实时数据处理
大多数瞳孔测
 表II
算法比较结果
V3T V3D V3SPP V5 V7
所有 0.938 (0.958) 0.977 (0.981) 0.971 (0.984) 0.974 (0.973) 0.976 (0.982)
无人机 0.915 (0.935) 0.968 (0.969) 0.954 (0.966) 0.964 (0.974) 0.964 (0.968)
机器人手臂 0.966 (0.991) 0.996 (0.995) 0.98 (0.994) 0.99 (0.995) 0.994 (0.996)
移动机器人 0.989 (0.995) 0.992 (0.995) 0.99 (0.995) 0.998 (0.995) 0.997 (0.996)
控制器 0.879 (0.911) 0.953 (0.964) 0.958 (0.98) 0.945 (0.929) 0.947 (0.966)
以及测试。相反，我们将它们分割为训练和验证的比例约为每个类别实例的80％，其余20％用于验证（一些图像可能具有2个或3个对象，而其他图像可能具有1个或0个）。这产生了684张训练图像和225张验证图像。表I代表数据集及其分割细节。
B. 算法比较
如II节所述，YOLO以其在多个版本中进行对象检测的速度和准确性而闻名。选择YOLO算法的家族，以实现实时检测为目标。

我们首先评估了在我们的特定检测问题中YOLOv3，YOLOv5和YOLOv7的性能。为此，我们选择了三个版本的YOLOv3：Tiny（V3T），默认（V3D）和SPP（V3SPP），以及大型版本的YOLOv5和YOLO7最大模型（e6e）。表II呈现了在每个预训练于MS COCO的模型上进行评估的结果，并在我们的收集到的数据集上进行了微调和评估。数据增强通过YOLO在模型训练过程中增加数据集的有效大小，从而提高模型的泛化能力。训练和验证都在Compute Canada集群上进行（[28]）。对于每个模型，我们在交集为0.5的边界框周围报告F1指标和平均精度（实践之间）。

接下来，我们需要能够在ROS中使用这些训练好的检测模型并进行调整，以便将检测信息与人的注视信息对齐。为此，我们决定研究公共可用的开源ROS包，如YOLOv3 [29]，YOLOv5 [30]和YOLOv7 [31]。通过使用我们的训练权重运行视频流通过每个模型，我们分别得到了8 FPS，3 FPS和1 FPS for YOLOv3，YOLOv5和YOLOv7。测试是在一个Intel® Xeon(R) E-2276M CPU @ 2.80GHz × 12 CPU，配备NVIDIA Quadro P620 512 CUDA核心GPU，4 GB GDDR5，16 GB DDR4 RAM运行Ubuntu 20.04.4 LTS和ROS Noetic进行的。

C. 凝视物体
在3D环境中本身很容易进行凝视跟踪。但是，结合动态3D空间中的凝视跟踪会使它变得更加困难。从视频流中检测到的物体周围有边框。通过添加相同图像参考系的目光坐标，我们可以推断出是什么人在看（例如，图2中的PS4控制器）。
图2。双摇杆4控制器上的凝视。一个人被要求连续10秒钟看着遥控器。粉红色表示目光。每个视频帧中的物体都带有预测的类别和置信度标签。

为了避免任何碰撞，人需要扫过周围的物体。为了判断人是否正在关注动作，我们添加了一个动态ROI（区域感兴趣）附加到场景中检测到的物体上。我们可以在图3中看到ROI（绿色）。我们使用由Dinu C. Gherman编写的函数来定义ROI，方法是给出图像中的点。
图3。ROI内的目光位置（绿色）。ROI是动态的，会跟随移动的对象。目光（粉红色）在这个位于无人机和机器人手臂之间的区域移动。

V. USER STUDY
A. 方法
本研究在室内进行，以保持更受控的环境。图4显示了环境地图。一个高3米 ceiling的大型长房间。参与者（n=8）驾驶一个无人机在一个机器人顶部（静止或运动，见条件下的图3）上飞行，目标是尽可能多次地绕着臂飞行无人机。生物物理测量，包括瞳孔直径、眼动和心率等。
 图4. 室内环境地图。黑色圆点表示参与者。黄色矩形带蓝色圆点点的是顶部装有Kinova臂的Dingo。蓝色线条是侧向路径，绿色线条是双向路径，红色曲线是斜道路径。粉色X代表CogniFLY无人机。
在执行每个任务时，从每个参与者那里记录了心率(ECG)。该研究分为5个任务，每个任务的持续时间为1分钟：
1) 在没有障碍物的情况下自由驾驶无人机（熟悉参与者）；
2) 在固定于Clearpath Dingo-D的垂直Kinova机器人手臂上驾驶无人机；
3) 在固定于Clearpath Dingo-D的垂直Kinova手臂上，以25厘米/秒的速度向前和后移动4米；
4) 在固定于Clearpath Dingo-D的垂直Kinova手臂上，以25厘米/秒的速度侧向移动4米；
5) 在固定于Clearpath Dingo-D的垂直Kinova手臂上，以斜道运动。
在实验开始之前，询问参与者关于他们驾驶无人机的经验。需要考虑这一点，因为这会对他们的表现产生很大影响。第一次驾驶无人机的人与飞行时间较长的人反射或熟悉程度不同。
在进行任务之前，参与者穿上了Polar H10心率胸带传感器，在衬衫下记录心率测量值和一副Tobii Pro Glasses 3眼动眼镜。然后，我们解释了CogniFLY无人机的控制方式[25]。参与者被指示站在地面上的标记上（图4中的黑色圆点）。为了确定每个参与者将观察到的区域的亮度基线，我们让参与者看10秒钟手中拿着的遥控器、无人机、Dingo-D和Kinova机器人手臂。
完成后，我们开始进行第一项任务。
在完成第一项任务（在区域内自由飞行无人机）后，参与者完成了第二项任务（静态条件）。完成后，参与者完成了第三项任务（前后移动）或第四项任务（侧向移动）。第三项和第四项任务是平衡的，因此所有参与者都做了这两个任务，但不是按照相同的顺序。最后，参与者完成了第五项任务（斜道运动）。在实验过程中，有些参与者（n=3）发生了无人机坠落（见图5. 分类器分布：至少检测到两个物体帧数占总帧数的比例（臂，基地，UAV）。对于最后一个三个任务，几乎每帧都有至少两个物体被检测到，足够计算出有意义的ROI。
B. 结果
平均而言，参与者在反向和正向（任务3）以及侧向（任务4）条件下最能在Kinova臂周围画圈，而在斜道（任务5）条件下最困难，这表明斜道条件最具挑战性。瞳孔直径测量值支持这一观点，因为在斜道条件下，参与者平均拥有最大的瞳孔直径（见图6）。心率变异性测量值（SDNN）在反向和正向以及斜道任务之间也显示出这种趋势。
反向 side2side slalom20.030.040.050.060.070.0SDNN
反向 side2side slalom-0.20.00.20.50.81.0Pupil variation (mm)
图6. 所有参与者认知负荷指标的平均值。然后，我们将数据划
 在所有任务中，除了侧向任务（见图7）。然而，这可能是因为额外的任务特定训练。由于平衡校正（没有考虑专家知识），四个新手中有三个在实验后期比大多数专家晚进行了侧向任务。专家组在各项任务中的表现类似，而新手组在侧向条件下的表现有所下降。

眼睛测量值在各组之间略有不同（见图8），专家在侧向条件下的眼睛直径最大，其次是双向条件，最后是侧向条件。新手在双向条件下的眼睛直径最大，其次是侧向，然后是侧向条件。这表明，专家在侧向条件下的认知负荷可能最高，而新手在双向条件下的认知负荷可能最高。

从图8可以看出，专家的视线转移次数较多，但注意力更加集中和有意图（参见图9中的热力图）。这可能表明，专家在操作无人机方面更为熟练，能够更有效地监控周围环境以完成任务，相比之下，新手则不太擅长。

为了更好地了解UAV飞行员如何操作，我们将眼睛和心率特征完全整合到了HRI4ROS管道中。我们提供了一套ROS节点来捕捉和处理这些传感器的实时数据，并构建了一个用于训练在飞行员前视摄像头 Feed上进行目标检测的数据集。我们的训练数据显示，新手在双向条件下的侧向任务表现最好，其次是侧向，然后是双向。

在心率变化指标方面，专家在双向条件下的标准差高于侧向或侧向条件。新手的标准差较高，但在侧向条件下较低，而在双向条件下的标准差最低。与眼睛和性能指标相似，这种结果模式显示出熟悉化的趋势，因为每个组在第四个组中最常进行的条件下，标准差增加（表示认知负荷降低）。

最后，如图8所示，专家的视线转移次数较多，但注意力更加集中和有意图（参见图9中的热力图）。这可能表明，专家在操作无人机方面更为熟练，能够更有效地监控周围环境以完成任务，相比之下，新手则不太擅长。

VII. 结论

为了更好地理解UAV飞行员如何操作，我们介绍了将眼睛和心率特征完整地整合到HRI4ROS管道中的方法。我们提供了一套ROS节点来捕捉和处理来自这些传感器的实时数据，并构建了一个用于训练在飞行员前视摄像头 Feed上进行目标检测的数据集。我们训练的数据显示，新手在双向条件下的侧向任务表现最好，其次是侧向，然后是双向。

在眼动和飞行参数方面，研究结果表明，专家在操作无人机时似乎更倾向于使用偏航控制，而新手则更倾向于使用其他策略。
 社会年度会议，第64卷，第1期，pp. 1145-1149，2020年。
[在线]. 可访问：https://doi.org/10.1177/1071181320641274
[5] R. Veerabhadrappa, I. Hettiarachchi, S. Hanoun, 和 D. Jia,
“在动态视觉任务中使用眼动动态进行有效策略标识和评估，”预印本 ，pp. 0–15，2021年。
[在线]. 可访问：https://doi.org/10.21203/rs.3.rs-778091/v1
[6] M. A. Shahab, M. U. Iqbal, B. Srinivasan, 和 R. Srinivasan, “使用眼动模式客观评估操作者培训的指标，”过程安全与环境保护 ，vol. 156, pp. 508-520, 2021年。 [在线]. 可访问：https://doi.org/10.1016/j.psep.2021.10.043
[7] G. Ziv, “飞行器中的眼神行为和视觉注意力：航空器视觉跟踪研究综述，”国际航空心理学杂志 ，vol. 26, no. 3-4, pp. 75–104, 2016年。 [在线]. 可访问：https://doi.org/10.1080/10508414.2017.1313096
[8] S. S. A. Zaidi, M. S. Ansari, A. Aslam, N. Kanwal, M. Asghar, 和 B. Lee, “现代基于深度学习的物体检测模型的调查，”数字信号处理 ，p. 103514, 2022。
[9] R. Girshick, J. Donahue, T. Darrell, 和 J. Malik, “丰富的特征层次用于准确的物体检测和语义分割，” inProceedings of the IEEE计算机视觉和模式识别会议 , 2014, pp. 580–587。
[10] K. He, X. Zhang, S. Ren, 和 J. Sun, “在深度卷积网络中进行视觉识别的空间金字塔池化，”IEEE图像处理与机器智能杂志 ，vol. 37, no. 9, pp. 1904–1916, 2015年。
[11] S. Qiao, L. Chen, 和 A. L. Yuille, “利用递归特征金字塔和可切换卷积的检测器，“CoRR”，vol. abs/2006.02334, 2020年。 [在线]. 可访问：https://arxiv.org/abs/2006.02334
[12] C.-Y . Wang, A. Bochkovskiy, 和 H.-Y . M. Liao, “Yolov7：可训练的免费库设置新的实时目标检测器状态，“2022。 [在线]. 可访问：https://arxiv.org/abs/2207.02696
[13] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, 和 A. C. Berg, “Ssd：单次多框检测器，“In Proceedings of the European Conference on Computer Vision . Springer, 2016, pp. 21–37。
[14] M. Tan, R. Pang, 和 Q. V . Le, “有效且高效的对象检测，“IEEE/CVF conferences on计算机视觉和模式识别 , 2020, pp. 10 781–10 790。
[15] J. Ruiz-del-Solar, P. Loncomilla, 和 N. Soto, “关于深度学习方法在机器人视觉中的应用的调查，”CoRR ，vol. abs/1803.10862, 2018年。 [在线]. 可访问：http://arxiv.org/abs/1803.10862
[16] Z. Zou, Z. Shi, Y . Guo, 和 J. Ye, “20年后的人造物体检测：一项回顾，”arXiv预印本 arXiv:1905.05055 , 2019。
[17] G. Br ˆone, B. Oben, 和 T. Goedem ´e, “分析移动眼动数据中更有效的策略：将目光数据与对象识别算法相结合，”PETMEI’11 - 第一届国际研讨会 on Pervasive Eye Tracking and Mobile Eye-Based Interaction , pp. 53–56, 2011。
[18] T. Callemein, K. Van Beeck, G. Br ˆone, 和 T. Goedem ´e, “基于眼动追踪的人与人交互研究的自动分析，“国际信息科学与系统应用会议 . Springer, 2018, pp. 499–509。
[19] L. Stubbemann, D. D  是非常非常重要的 
